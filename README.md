# ğŸ§  Dynamic AI Customer Support Backend

An end-to-end **AI-powered customer support backend** built with **FastAPI**, combining **offline ingestion**, **vector search (FAISS)**, **intent detection**, **human feature extraction**, **LLM reasoning**, and **response strategy selection**.

This system is designed using **clean modular architecture**, separating **offline processing** and **online query execution** for scalability and maintainability.

---

## ğŸš€ Key Features

* ğŸ”¹ Offline document ingestion & preprocessing
* ğŸ”¹ Chunking + embeddings using Sentence Transformers
* ğŸ”¹ FAISS-based vector similarity search
* ğŸ”¹ Intent & emotion detection
* ğŸ”¹ Human behavior feature extraction
* ğŸ”¹ Context-aware retrieval routing
* ğŸ”¹ LLM-powered response generation
* ğŸ”¹ Answer validation to reduce hallucinations
* ğŸ”¹ Strategy-based response selection
* ğŸ”¹ FastAPI REST interface

---

## ğŸ—ï¸ System Architecture Overview

### Offline Timeline (One-Time / Batch Process)

1. Load raw text data
2. Clean & preprocess documents
3. Chunk large documents
4. Generate embeddings
5. Enrich metadata
6. Store vectors in FAISS index

### Online Timeline (Per User Query)

1. Preprocess user query
2. Extract human behavior features
3. Detect intent & emotion
4. Route retrieval strategy
5. Retrieve relevant chunks
6. Assemble contextual prompt
7. Generate response using LLM
8. Validate answer confidence
9. Return final response

---

## ğŸ“ Project Structure

```text
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # Application entry point
â”‚   â”œâ”€â”€ __init__.py
â”‚
â”‚   â”œâ”€â”€ ingestion/             # Offline ingestion pipeline
â”‚   â”‚   â”œâ”€â”€ data_load.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â”œâ”€â”€ embedding.py
â”‚   â”‚   â”œâ”€â”€ metadata_enricher.py
â”‚   â”‚   â”œâ”€â”€ ingestion_manager.py
â”‚   â”‚   â”œâ”€â”€ run_preprocessing.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚
â”‚   â”œâ”€â”€ intent_detection/      # Intent & emotion detection
â”‚   â”‚   â”œâ”€â”€ intent_classifier.py
â”‚   â”‚   â”œâ”€â”€ intent_features.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚
â”‚   â”œâ”€â”€ query_pipeline/        # Online query processing
â”‚   â”‚   â”œâ”€â”€ query_preprocess.py
â”‚   â”‚   â”œâ”€â”€ human_features.py
â”‚   â”‚   â”œâ”€â”€ query_embed.py
â”‚   â”‚   â”œâ”€â”€ context_assembler.py
â”‚   â”‚   â”œâ”€â”€ retrieval_router.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚
â”‚   â”œâ”€â”€ vector_store/           # Vector storage layer
â”‚   â”‚   â”œâ”€â”€ faiss_index.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚
â”‚   â”œâ”€â”€ reasoning/              # LLM reasoning
â”‚   â”‚   â”œâ”€â”€ llm_reasoner.py
â”‚   â”‚   â”œâ”€â”€ response_generator.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚
â”‚   â”œâ”€â”€ response_strategy/      # Response style selection
â”‚   â”‚   â”œâ”€â”€ response_router.py
â”‚   â”‚   â”œâ”€â”€ response_strategy.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚
â”‚   â”œâ”€â”€ validation/             # Answer validation
â”‚   â”‚   â”œâ”€â”€ answer_validator.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ training_data.txt   # Knowledge base
```

---

## ğŸ§© Core Components Explained

### ğŸ”¹ Ingestion Pipeline (`ingestion/`)

Handles offline data preparation:

* Reads large text files
* Cleans & chunks content
* Generates embeddings
* Enriches metadata
* Prepares data for vector storage

### ğŸ”¹ Intent Detection (`intent_detection/`)

Detects:

* User intent (greeting, question, complaint, etc.)
* Emotional tone (angry, neutral, urgent)

### ğŸ”¹ Query Pipeline (`query_pipeline/`)

Online query execution:

* Cleans user input
* Extracts human behavioral features
* Embeds queries
* Retrieves relevant context

### ğŸ”¹ Vector Store (`vector_store/`)

* FAISS-based similarity search
* Efficient nearest-neighbor lookup

### ğŸ”¹ Reasoning Engine (`reasoning/`)

* Uses LLM to generate answers from retrieved context
* Applies system prompts dynamically

### ğŸ”¹ Response Strategy (`response_strategy/`)

* Chooses response tone (polite, empathetic, concise, etc.)
* Adjusts based on intent & emotion

### ğŸ”¹ Validation (`validation/`)

* Ensures answers are grounded in context
* Reduces hallucinations via confidence scoring

---

## âš™ï¸ Tech Stack

* **Backend Framework:** FastAPI
* **Embeddings:** Sentence Transformers (`all-MiniLM-L6-v2`)
* **Vector Search:** FAISS
* **LLM:** TinyLlama 1.1B Chat
* **Data Processing:** NumPy
* **API Schema:** Pydantic

---

## â–¶ï¸ Running the Application

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Set Training Data Path

Update in `app/main.py`:

```python
DATA_PATH = "/path/to/training_data.txt"
```

### 3ï¸âƒ£ Start the Server

```bash
uvicorn app.main:app --reload
```

### 4ï¸âƒ£ API Endpoints

* **Health Check**

```http
GET /
```

* **Query Chatbot**

```http
POST /query
Content-Type: application/json

{
  "user_query": "How do I reset my password?"
}
```

---

## ğŸ§ª Example Response Flow

1. User sends a query
2. Intent + emotion detected
3. Context retrieved from FAISS
4. LLM generates response
5. Validator checks confidence
6. Final answer returned

---

## ğŸ“Œ Future Improvements

* Streaming responses
* Multi-language support
* Persistent session memory
* Redis-based caching
* Async ingestion
* Hybrid search (BM25 + vectors)

---

## ğŸ‘¨â€ğŸ’» Author

**Jenish Shekhada**
AI Engineer | GenAI | RAG Systems | FastAPI


